"""
Financial Risk Management Project - Simplified Version
======================================================

This project implements a comprehensive financial risk management system
following the methodology outlined in the project requirements:

Phase 1: Data Collection and Cleaning
Phase 2: Exploratory Data Analysis (EDA)
Phase 3: Predictive Modeling

Author: Financial Risk Management Team
Date: October 31, 2025
"""

import pandas as pd
import numpy as np
import warnings
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
import json
import os

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

class FinancialRiskManager:
    """
    A comprehensive Financial Risk Management system that handles data loading,
    cleaning, analysis, and predictive modeling for loan default prediction.
    """
    
    def __init__(self, accepted_data_path, rejected_data_path):
        """
        Initialize the Financial Risk Manager
        
        Parameters:
        accepted_data_path (str): Path to accepted loans dataset
        rejected_data_path (str): Path to rejected loans dataset
        """
        self.accepted_data_path = accepted_data_path
        self.rejected_data_path = rejected_data_path
        self.accepted_data = None
        self.rejected_data = None
        self.combined_data = None
        self.processed_data = None
        self.models = {}
        self.analysis_results = {}
        
    def load_data(self, sample_size=None):
        """
        Load and combine accepted and rejected loan datasets
        
        Parameters:
        sample_size (int): Optional sample size for large datasets
        """
        print("üîÑ Phase 1: Data Collection and Loading")
        print("=" * 50)
        
        try:
            # Load accepted loans data
            print("üìä Loading accepted loans data...")
            if sample_size:
                self.accepted_data = pd.read_csv(self.accepted_data_path, nrows=sample_size)
            else:
                self.accepted_data = pd.read_csv(self.accepted_data_path, low_memory=False)
            
            self.accepted_data['loan_status_binary'] = 0  # 0 for accepted (good)
            print(f"‚úÖ Loaded {len(self.accepted_data):,} accepted loan records")
            
            # Load rejected loans data  
            print("üìä Loading rejected loans data...")
            if sample_size:
                self.rejected_data = pd.read_csv(self.rejected_data_path, nrows=sample_size)
            else:
                self.rejected_data = pd.read_csv(self.rejected_data_path, low_memory=False)
                
            self.rejected_data['loan_status_binary'] = 1  # 1 for rejected (risky)
            print(f"‚úÖ Loaded {len(self.rejected_data):,} rejected loan records")
            
            # Get common columns for merging
            common_columns = list(set(self.accepted_data.columns) & set(self.rejected_data.columns))
            print(f"üìã Found {len(common_columns)} common columns between datasets")
            
            # Combine datasets using common columns
            accepted_subset = self.accepted_data[common_columns]
            rejected_subset = self.rejected_data[common_columns]
            
            self.combined_data = pd.concat([accepted_subset, rejected_subset], 
                                         ignore_index=True, sort=False)
            
            print(f"üéØ Combined dataset shape: {self.combined_data.shape}")
            print(f"üìà Total records: {len(self.combined_data):,}")
            
            return self.combined_data
            
        except Exception as e:
            print(f"‚ùå Error loading data: {str(e)}")
            return None
    
    def clean_data(self):
        """
        Clean and preprocess the combined dataset
        """
        print("\nüßπ Data Cleaning and Preprocessing")
        print("=" * 40)
        
        if self.combined_data is None:
            print("‚ùå No data loaded. Please load data first.")
            return None
        
        # Create a copy for processing
        self.processed_data = self.combined_data.copy()
        
        # Display initial data info
        print(f"üìä Initial dataset shape: {self.processed_data.shape}")
        print(f"üîç Missing values analysis:")
        missing_data = self.processed_data.isnull().sum()
        missing_percentage = (missing_data / len(self.processed_data)) * 100
        missing_df = pd.DataFrame({
            'Missing_Count': missing_data,
            'Missing_Percentage': missing_percentage
        }).sort_values('Missing_Percentage', ascending=False)
        
        # Show top missing columns
        print("Top 10 columns with missing values:")
        print(missing_df.head(10))
        
        # Remove columns with high missing values (>80%)
        high_missing_cols = missing_df[missing_df['Missing_Percentage'] > 80].index.tolist()
        if high_missing_cols:
            print(f"\nüóëÔ∏è Removing {len(high_missing_cols)} columns with >80% missing values")
            self.processed_data = self.processed_data.drop(columns=high_missing_cols)
        
        # Handle numeric columns
        numeric_columns = self.processed_data.select_dtypes(include=[np.number]).columns.tolist()
        if 'loan_status_binary' in numeric_columns:
            numeric_columns.remove('loan_status_binary')
        
        print(f"üî¢ Processing {len(numeric_columns)} numeric columns...")
        
        # Impute missing values for numeric columns
        if numeric_columns:
            numeric_imputer = SimpleImputer(strategy='median')
            self.processed_data[numeric_columns] = numeric_imputer.fit_transform(
                self.processed_data[numeric_columns]
            )
        
        # Handle categorical columns
        categorical_columns = self.processed_data.select_dtypes(include=['object']).columns.tolist()
        print(f"üìù Processing {len(categorical_columns)} categorical columns...")
        
        if categorical_columns:
            categorical_imputer = SimpleImputer(strategy='most_frequent')
            self.processed_data[categorical_columns] = categorical_imputer.fit_transform(
                self.processed_data[categorical_columns]
            )
        
        # Remove duplicates
        initial_rows = len(self.processed_data)
        self.processed_data = self.processed_data.drop_duplicates()
        removed_duplicates = initial_rows - len(self.processed_data)
        
        if removed_duplicates > 0:
            print(f"üóëÔ∏è Removed {removed_duplicates:,} duplicate rows")
        
        print(f"‚úÖ Final cleaned dataset shape: {self.processed_data.shape}")
        print(f"üéØ Data cleaning completed successfully!")
        
        return self.processed_data
    
    def exploratory_data_analysis(self):
        """
        Perform comprehensive Exploratory Data Analysis
        """
        print("\nüìä Phase 2: Exploratory Data Analysis (EDA)")
        print("=" * 50)
        
        if self.processed_data is None:
            print("‚ùå No processed data available. Please clean data first.")
            return
        
        # Create analysis results directory
        if not os.path.exists("financial_analysis_results"):
            os.makedirs("financial_analysis_results")
        
        # 1. Basic dataset statistics
        print("üìà Dataset Overview:")
        print(f"Total Records: {len(self.processed_data):,}")
        print(f"Total Features: {self.processed_data.shape[1]}")
        
        # 2. Target variable distribution
        target_distribution = self.processed_data['loan_status_binary'].value_counts()
        print(f"\nüéØ Loan Status Distribution:")
        print(f"Accepted Loans (0): {target_distribution[0]:,} ({target_distribution[0]/len(self.processed_data)*100:.1f}%)")
        print(f"Rejected Loans (1): {target_distribution[1]:,} ({target_distribution[1]/len(self.processed_data)*100:.1f}%)")
        
        # 3. Numeric feature analysis
        numeric_columns = self.processed_data.select_dtypes(include=[np.number]).columns.tolist()
        if 'loan_status_binary' in numeric_columns:
            numeric_columns.remove('loan_status_binary')
        
        print(f"\nüî¢ Analyzing {len(numeric_columns)} numeric features...")
        
        # Calculate correlations with target
        correlation_with_target = {}
        for col in numeric_columns:
            try:
                corr = self.processed_data[col].corr(self.processed_data['loan_status_binary'])
                if not pd.isna(corr):
                    correlation_with_target[col] = abs(corr)
            except:
                continue
        
        # Sort correlations
        sorted_correlations = sorted(correlation_with_target.items(), 
                                   key=lambda x: x[1], reverse=True)
        
        print("\nüîó Top 10 Features Correlated with Loan Status:")
        for i, (feature, corr) in enumerate(sorted_correlations[:10], 1):
            print(f"{i:2d}. {feature}: {corr:.4f}")
        
        # 4. Summary statistics by loan status
        print("\nüìä Summary Statistics by Loan Status:")
        if len(numeric_columns) > 0:
            top_features = [item[0] for item in sorted_correlations[:5]]
            summary_stats = self.processed_data.groupby('loan_status_binary')[top_features].agg([
                'count', 'mean', 'median', 'std'
            ]).round(2)
            print(summary_stats)
        
        # 5. Risk factor identification
        print("\n‚ö†Ô∏è Key Risk Factors Identified:")
        risk_factors = []
        for feature, corr in sorted_correlations[:5]:
            accepted_mean = self.processed_data[self.processed_data['loan_status_binary']==0][feature].mean()
            rejected_mean = self.processed_data[self.processed_data['loan_status_binary']==1][feature].mean()
            
            risk_direction = "Higher" if rejected_mean > accepted_mean else "Lower"
            risk_factors.append({
                'feature': feature,
                'correlation': corr,
                'accepted_mean': accepted_mean,
                'rejected_mean': rejected_mean,
                'risk_direction': risk_direction
            })
            
            print(f"‚Ä¢ {feature}: {risk_direction} values associated with rejection risk")
            print(f"  - Accepted loans avg: {accepted_mean:.2f}")
            print(f"  - Rejected loans avg: {rejected_mean:.2f}")
            print(f"  - Correlation strength: {corr:.4f}")
        
        # Store analysis results
        self.analysis_results = {
            'dataset_overview': {
                'total_records': len(self.processed_data),
                'total_features': self.processed_data.shape[1],
                'accepted_loans': int(target_distribution[0]),
                'rejected_loans': int(target_distribution[1]),
                'rejection_rate': float(target_distribution[1]/len(self.processed_data))
            },
            'top_correlations': dict(sorted_correlations[:10]),
            'risk_factors': risk_factors
        }
        
        # Save analysis to file
        with open('financial_analysis_results/eda_summary.json', 'w') as f:
            json.dump(self.analysis_results, f, indent=2, default=str)
        
        print(f"\n‚úÖ EDA completed! Results saved to 'financial_analysis_results' directory")
        return self.analysis_results
    
    def prepare_modeling_data(self):
        """
        Prepare data for machine learning modeling
        """
        print("\nüîß Preparing Data for Machine Learning")
        print("=" * 40)
        
        if self.processed_data is None:
            print("‚ùå No processed data available.")
            return None, None, None, None
        
        # Select numeric features for modeling
        numeric_features = self.processed_data.select_dtypes(include=[np.number]).columns.tolist()
        if 'loan_status_binary' in numeric_features:
            numeric_features.remove('loan_status_binary')
        
        # Limit features to prevent overfitting and improve performance
        selected_features = numeric_features[:15]  # Top 15 numeric features
        
        print(f"üìä Selected {len(selected_features)} features for modeling:")
        for i, feature in enumerate(selected_features, 1):
            print(f"{i:2d}. {feature}")
        
        # Prepare feature matrix and target
        X = self.processed_data[selected_features]
        y = self.processed_data['loan_status_binary']
        
        # Handle any remaining missing values
        X = X.fillna(X.median())
        
        # Scale features
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        X_scaled = pd.DataFrame(X_scaled, columns=selected_features)
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print(f"‚úÖ Data prepared successfully!")
        print(f"üìà Training set: {X_train.shape[0]:,} samples")
        print(f"üìä Test set: {X_test.shape[0]:,} samples")
        print(f"üéØ Class distribution in training:")
        print(f"   - Accepted (0): {(y_train == 0).sum():,} ({(y_train == 0).mean()*100:.1f}%)")
        print(f"   - Rejected (1): {(y_train == 1).sum():,} ({(y_train == 1).mean()*100:.1f}%)")
        
        return X_train, X_test, y_train, y_test
    
    def train_models(self, X_train, X_test, y_train, y_test):
        """
        Train and evaluate machine learning models
        """
        print("\nü§ñ Phase 3: Predictive Modeling")
        print("=" * 40)
        
        # Initialize models
        models = {
            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
            'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100)
        }
        
        results = {}
        
        for model_name, model in models.items():
            print(f"\nüîÑ Training {model_name}...")
            
            # Train model
            model.fit(X_train, y_train)
            
            # Make predictions
            y_pred_train = model.predict(X_train)
            y_pred_test = model.predict(X_test)
            
            # Calculate metrics
            train_accuracy = accuracy_score(y_train, y_pred_train)
            test_accuracy = accuracy_score(y_test, y_pred_test)
            
            # Confusion matrix
            cm = confusion_matrix(y_test, y_pred_test)
            
            # Classification report
            class_report = classification_report(y_test, y_pred_test, output_dict=True)
            
            # Store results
            results[model_name] = {
                'model': model,
                'train_accuracy': train_accuracy,
                'test_accuracy': test_accuracy,
                'confusion_matrix': cm.tolist(),
                'classification_report': class_report
            }
            
            print(f"‚úÖ {model_name} Results:")
            print(f"   üìà Training Accuracy: {train_accuracy:.4f}")
            print(f"   üìä Test Accuracy: {test_accuracy:.4f}")
            print(f"   üéØ Generalization Gap: {(train_accuracy - test_accuracy):.4f}")
            
            print("\nüìã Confusion Matrix:")
            print(f"   True Negative (Correct Accepted): {cm[0,0]:,}")
            print(f"   False Positive (Wrong Rejected): {cm[0,1]:,}")
            print(f"   False Negative (Wrong Accepted): {cm[1,0]:,}")
            print(f"   True Positive (Correct Rejected): {cm[1,1]:,}")
            
            print(f"\nüìä Classification Report:")
            print(f"   Precision (Accepted): {class_report['0']['precision']:.4f}")
            print(f"   Precision (Rejected): {class_report['1']['precision']:.4f}")
            print(f"   Recall (Accepted): {class_report['0']['recall']:.4f}")
            print(f"   Recall (Rejected): {class_report['1']['recall']:.4f}")
            print(f"   F1-Score (Macro Avg): {class_report['macro avg']['f1-score']:.4f}")
        
        self.models = results
        return results
    
    def generate_insights_and_recommendations(self):
        """
        Generate actionable insights and business recommendations
        """
        print("\nüí° Business Insights and Recommendations")
        print("=" * 50)
        
        # Model performance insights
        best_model = max(self.models.keys(), 
                        key=lambda x: self.models[x]['test_accuracy'])
        best_accuracy = self.models[best_model]['test_accuracy']
        
        print("üèÜ MODEL PERFORMANCE:")
        print(f"   Best Model: {best_model}")
        print(f"   Accuracy: {best_accuracy:.2%}")
        print(f"   Business Impact: Can correctly identify {best_accuracy:.1%} of risk cases")
        
        # Risk insights
        rejection_rate = self.analysis_results['dataset_overview']['rejection_rate']
        total_loans = self.analysis_results['dataset_overview']['total_records']
        
        print(f"\nüìä RISK PROFILE:")
        print(f"   Overall Rejection Rate: {rejection_rate:.2%}")
        print(f"   Total Applications Analyzed: {total_loans:,}")
        print(f"   High-Risk Applications: {int(total_loans * rejection_rate):,}")
        
        # Feature importance insights
        print(f"\nüîç KEY RISK FACTORS:")
        for i, factor in enumerate(self.analysis_results['risk_factors'][:5], 1):
            print(f"   {i}. {factor['feature']}")
            print(f"      - Risk Direction: {factor['risk_direction']} values = Higher risk")
            print(f"      - Predictive Strength: {factor['correlation']:.3f}")
        
        # Business recommendations
        recommendations = [
            "üéØ IMPLEMENTATION RECOMMENDATIONS:",
            f"   1. Deploy {best_model} for automated loan screening",
            "   2. Focus monitoring on top 5 risk factors identified",
            "   3. Implement tiered approval process based on risk scores",
            "   4. Set up real-time alerts for high-risk applications",
            "   5. Regularly retrain models with new loan performance data",
            "",
            "üí∞ BUSINESS VALUE:",
            f"   - Reduce manual review time by {best_accuracy:.0%}",
            "   - Improve risk detection accuracy",
            "   - Enable faster loan approval decisions",
            "   - Minimize potential default losses",
            "",
            "‚ö†Ô∏è RISK MANAGEMENT:",
            "   - Monitor model performance monthly",
            "   - Validate predictions with actual outcomes",
            "   - Maintain human oversight for edge cases",
            "   - Update risk thresholds based on business needs"
        ]
        
        print("\n" + "\n".join(recommendations))
        
        # Save insights
        insights_data = {
            'model_performance': {
                'best_model': best_model,
                'accuracy': float(best_accuracy),
                'all_models': {name: float(results['test_accuracy']) 
                             for name, results in self.models.items()}
            },
            'risk_profile': self.analysis_results['dataset_overview'],
            'key_risk_factors': self.analysis_results['risk_factors'][:5],
            'recommendations': recommendations
        }
        
        with open('financial_analysis_results/business_insights.json', 'w') as f:
            json.dump(insights_data, f, indent=2, default=str)
        
        print(f"\n‚úÖ Insights saved to 'financial_analysis_results/business_insights.json'")
        return insights_data
    
    def run_complete_analysis(self, sample_size=None):
        """
        Run the complete financial risk management analysis pipeline
        """
        print("üöÄ FINANCIAL RISK MANAGEMENT SYSTEM")
        print("=" * 60)
        print("Starting comprehensive risk analysis pipeline...")
        print()
        
        try:
            # Phase 1: Data Loading and Cleaning
            self.load_data(sample_size)
            if self.combined_data is None:
                return
            
            self.clean_data()
            if self.processed_data is None:
                return
            
            # Phase 2: Exploratory Data Analysis
            self.exploratory_data_analysis()
            
            # Phase 3: Predictive Modeling
            X_train, X_test, y_train, y_test = self.prepare_modeling_data()
            if X_train is None:
                return
            
            self.train_models(X_train, X_test, y_train, y_test)
            
            # Phase 4: Business Insights
            self.generate_insights_and_recommendations()
            
            print("\nüéâ ANALYSIS COMPLETED SUCCESSFULLY!")
            print("=" * 50)
            print("üìÅ All results saved in 'financial_analysis_results' directory")
            print("üìä Check the generated analysis files and reports")
            print("üí° Review business insights for actionable recommendations")
            print("\nüîë KEY FILES GENERATED:")
            print("   - eda_summary.json: Exploratory data analysis results")
            print("   - business_insights.json: Business recommendations")
            print("\nüéØ NEXT STEPS:")
            print("   1. Review the key risk factors identified")
            print("   2. Implement the recommended model in production")
            print("   3. Set up monitoring for ongoing risk assessment")
            print("   4. Plan regular model retraining schedule")
            
        except Exception as e:
            print(f"‚ùå Error during analysis: {str(e)}")
            import traceback
            traceback.print_exc()

# Main execution
if __name__ == "__main__":
    # Initialize the Financial Risk Manager
    risk_manager = FinancialRiskManager(
        accepted_data_path="accepted_2007_to_2018q4.csv/accepted_2007_to_2018Q4.csv",
        rejected_data_path="rejected_2007_to_2018q4.csv/rejected_2007_to_2018Q4.csv"
    )
    
    # Run complete analysis
    # Using sample_size for faster processing with large datasets
    # For full analysis, remove the sample_size parameter
    print("üîß Note: Using sample of 50,000 records for faster processing")
    print("üîß For full dataset analysis, modify the sample_size parameter")
    print()
    
    risk_manager.run_complete_analysis(sample_size=50000)