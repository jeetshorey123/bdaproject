"""
Financial Risk Management Project - Simplified Version
======================================================

This project implements a comprehensive financial risk management system
following the methodology outlined in the project requirements:

Phase 1: Data Collection and Cleaning
Phase 2: Exploratory Data Analysis (EDA)
Phase 3: Predictive Modeling

Author: Financial Risk Management Team
Date: October 31, 2025
"""

import pandas as pd
import numpy as np
import warnings
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
import json
import os

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

class FinancialRiskManager:
    """
    A comprehensive Financial Risk Management system that handles data loading,
    cleaning, analysis, and predictive modeling for loan default prediction.
    """
    
    def __init__(self, accepted_data_path, rejected_data_path):
        """
        Initialize the Financial Risk Manager
        
        Parameters:
        accepted_data_path (str): Path to accepted loans dataset
        rejected_data_path (str): Path to rejected loans dataset
        """
        self.accepted_data_path = accepted_data_path
        self.rejected_data_path = rejected_data_path
        self.accepted_data = None
        self.rejected_data = None
        self.combined_data = None
        self.processed_data = None
        self.models = {}
        self.analysis_results = {}
        
    def load_data(self, sample_size=None):
        """
        Load and combine accepted and rejected loan datasets
        
        Parameters:
        sample_size (int): Optional sample size for large datasets
        """
        print("ğŸ”„ Phase 1: Data Collection and Loading")
        print("=" * 50)
        
        try:
            # Load accepted loans data
            print("ğŸ“Š Loading accepted loans data...")
            if sample_size:
                self.accepted_data = pd.read_csv(self.accepted_data_path, nrows=sample_size)
            else:
                self.accepted_data = pd.read_csv(self.accepted_data_path, low_memory=False)
            
            self.accepted_data['loan_status_binary'] = 0  # 0 for accepted (good)
            print(f"âœ… Loaded {len(self.accepted_data):,} accepted loan records")
            
            # Load rejected loans data  
            print("ğŸ“Š Loading rejected loans data...")
            if sample_size:
                self.rejected_data = pd.read_csv(self.rejected_data_path, nrows=sample_size)
            else:
                self.rejected_data = pd.read_csv(self.rejected_data_path, low_memory=False)
                
            self.rejected_data['loan_status_binary'] = 1  # 1 for rejected (risky)
            print(f"âœ… Loaded {len(self.rejected_data):,} rejected loan records")
            
            # Get common columns for merging
            common_columns = list(set(self.accepted_data.columns) & set(self.rejected_data.columns))
            print(f"ğŸ“‹ Found {len(common_columns)} common columns between datasets")
            
            # Combine datasets using common columns
            accepted_subset = self.accepted_data[common_columns]
            rejected_subset = self.rejected_data[common_columns]
            
            self.combined_data = pd.concat([accepted_subset, rejected_subset], 
                                         ignore_index=True, sort=False)
            
            print(f"ğŸ¯ Combined dataset shape: {self.combined_data.shape}")
            print(f"ğŸ“ˆ Total records: {len(self.combined_data):,}")
            
            return self.combined_data
            
        except Exception as e:
            print(f"âŒ Error loading data: {str(e)}")
            return None
    
    def clean_data(self):
        """
        Clean and preprocess the combined dataset
        """
        print("\nğŸ§¹ Data Cleaning and Preprocessing")
        print("=" * 40)
        
        if self.combined_data is None:
            print("âŒ No data loaded. Please load data first.")
            return None
        
        # Create a copy for processing
        self.processed_data = self.combined_data.copy()
        
        # Display initial data info
        print(f"ğŸ“Š Initial dataset shape: {self.processed_data.shape}")
        print(f"ğŸ” Missing values analysis:")
        missing_data = self.processed_data.isnull().sum()
        missing_percentage = (missing_data / len(self.processed_data)) * 100
        missing_df = pd.DataFrame({
            'Missing_Count': missing_data,
            'Missing_Percentage': missing_percentage
        }).sort_values('Missing_Percentage', ascending=False)
        
        # Show top missing columns
        print("Top 10 columns with missing values:")
        print(missing_df.head(10))
        
        # Remove columns with high missing values (>80%)
        high_missing_cols = missing_df[missing_df['Missing_Percentage'] > 80].index.tolist()
        if high_missing_cols:
            print(f"\nğŸ—‘ï¸ Removing {len(high_missing_cols)} columns with >80% missing values")
            self.processed_data = self.processed_data.drop(columns=high_missing_cols)
        
        # Handle numeric columns
        numeric_columns = self.processed_data.select_dtypes(include=[np.number]).columns.tolist()
        if 'loan_status_binary' in numeric_columns:
            numeric_columns.remove('loan_status_binary')
        
        print(f"ğŸ”¢ Processing {len(numeric_columns)} numeric columns...")
        
        # Impute missing values for numeric columns
        if numeric_columns:
            numeric_imputer = SimpleImputer(strategy='median')
            self.processed_data[numeric_columns] = numeric_imputer.fit_transform(
                self.processed_data[numeric_columns]
            )
        
        # Handle categorical columns
        categorical_columns = self.processed_data.select_dtypes(include=['object']).columns.tolist()
        print(f"ğŸ“ Processing {len(categorical_columns)} categorical columns...")
        
        if categorical_columns:
            categorical_imputer = SimpleImputer(strategy='most_frequent')
            self.processed_data[categorical_columns] = categorical_imputer.fit_transform(
                self.processed_data[categorical_columns]
            )
        
        # Remove duplicates
        initial_rows = len(self.processed_data)
        self.processed_data = self.processed_data.drop_duplicates()
        removed_duplicates = initial_rows - len(self.processed_data)
        
        if removed_duplicates > 0:
            print(f"ğŸ—‘ï¸ Removed {removed_duplicates:,} duplicate rows")
        
        print(f"âœ… Final cleaned dataset shape: {self.processed_data.shape}")
        print(f"ğŸ¯ Data cleaning completed successfully!")
        
        return self.processed_data
    
    def exploratory_data_analysis(self):
        """
        Perform comprehensive Exploratory Data Analysis
        """
        print("\nğŸ“Š Phase 2: Exploratory Data Analysis (EDA)")
        print("=" * 50)
        
        if self.processed_data is None:
            print("âŒ No processed data available. Please clean data first.")
            return
        
        # Create analysis results directory
        if not os.path.exists("financial_analysis_results"):
            os.makedirs("financial_analysis_results")
        
        # 1. Basic dataset statistics
        print("ğŸ“ˆ Dataset Overview:")
        print(f"Total Records: {len(self.processed_data):,}")
        print(f"Total Features: {self.processed_data.shape[1]}")
        
        # 2. Target variable distribution
        target_distribution = self.processed_data['loan_status_binary'].value_counts()
        print(f"\nğŸ¯ Loan Status Distribution:")
        print(f"Accepted Loans (0): {target_distribution[0]:,} ({target_distribution[0]/len(self.processed_data)*100:.1f}%)")
        print(f"Rejected Loans (1): {target_distribution[1]:,} ({target_distribution[1]/len(self.processed_data)*100:.1f}%)")
        
        # 3. Numeric feature analysis
        numeric_columns = self.processed_data.select_dtypes(include=[np.number]).columns.tolist()
        if 'loan_status_binary' in numeric_columns:
            numeric_columns.remove('loan_status_binary')
        
        print(f"\nğŸ”¢ Analyzing {len(numeric_columns)} numeric features...")
        
        # Calculate correlations with target
        correlation_with_target = {}
        for col in numeric_columns:
            try:
                corr = self.processed_data[col].corr(self.processed_data['loan_status_binary'])
                if not pd.isna(corr):
                    correlation_with_target[col] = abs(corr)
            except:
                continue
        
        # Sort correlations
        sorted_correlations = sorted(correlation_with_target.items(), 
                                   key=lambda x: x[1], reverse=True)
        
        print("\nğŸ”— Top 10 Features Correlated with Loan Status:")
        for i, (feature, corr) in enumerate(sorted_correlations[:10], 1):
            print(f"{i:2d}. {feature}: {corr:.4f}")
        
        # 4. Summary statistics by loan status
        print("\nğŸ“Š Summary Statistics by Loan Status:")
        if len(numeric_columns) > 0:
            top_features = [item[0] for item in sorted_correlations[:5]]
            summary_stats = self.processed_data.groupby('loan_status_binary')[top_features].agg([
                'count', 'mean', 'median', 'std'
            ]).round(2)
            print(summary_stats)
        
        # 5. Risk factor identification
        print("\nâš ï¸ Key Risk Factors Identified:")
        risk_factors = []
        for feature, corr in sorted_correlations[:5]:
            accepted_mean = self.processed_data[self.processed_data['loan_status_binary']==0][feature].mean()
            rejected_mean = self.processed_data[self.processed_data['loan_status_binary']==1][feature].mean()
            
            risk_direction = "Higher" if rejected_mean > accepted_mean else "Lower"
            risk_factors.append({
                'feature': feature,
                'correlation': corr,
                'accepted_mean': accepted_mean,
                'rejected_mean': rejected_mean,
                'risk_direction': risk_direction
            })
            
            print(f"â€¢ {feature}: {risk_direction} values associated with rejection risk")
            print(f"  - Accepted loans avg: {accepted_mean:.2f}")
            print(f"  - Rejected loans avg: {rejected_mean:.2f}")
            print(f"  - Correlation strength: {corr:.4f}")
        
        # Store analysis results
        self.analysis_results = {
            'dataset_overview': {
                'total_records': len(self.processed_data),
                'total_features': self.processed_data.shape[1],
                'accepted_loans': int(target_distribution[0]),
                'rejected_loans': int(target_distribution[1]),
                'rejection_rate': float(target_distribution[1]/len(self.processed_data))
            },
            'top_correlations': dict(sorted_correlations[:10]),
            'risk_factors': risk_factors
        }
        
        # Save analysis to file
        with open('financial_analysis_results/eda_summary.json', 'w') as f:
            json.dump(self.analysis_results, f, indent=2, default=str)
        
        print(f"\nâœ… EDA completed! Results saved to 'financial_analysis_results' directory")
        return self.analysis_results
    
    def prepare_modeling_data(self):
        """
        Prepare data for machine learning modeling
        """
        print("\nğŸ”§ Preparing Data for Machine Learning")
        print("=" * 40)
        
        if self.processed_data is None:
            print("âŒ No processed data available.")
            return None, None, None, None
        
        # Select numeric features for modeling
        numeric_features = self.processed_data.select_dtypes(include=[np.number]).columns.tolist()
        if 'loan_status_binary' in numeric_features:
            numeric_features.remove('loan_status_binary')
        
        # Limit features to prevent overfitting and improve performance
        selected_features = numeric_features[:15]  # Top 15 numeric features
        
        print(f"ğŸ“Š Selected {len(selected_features)} features for modeling:")
        for i, feature in enumerate(selected_features, 1):
            print(f"{i:2d}. {feature}")
        
        # Prepare feature matrix and target
        X = self.processed_data[selected_features]
        y = self.processed_data['loan_status_binary']
        
        # Handle any remaining missing values
        X = X.fillna(X.median())
        
        # Scale features
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        X_scaled = pd.DataFrame(X_scaled, columns=selected_features)
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print(f"âœ… Data prepared successfully!")
        print(f"ğŸ“ˆ Training set: {X_train.shape[0]:,} samples")
        print(f"ğŸ“Š Test set: {X_test.shape[0]:,} samples")
        print(f"ğŸ¯ Class distribution in training:")
        print(f"   - Accepted (0): {(y_train == 0).sum():,} ({(y_train == 0).mean()*100:.1f}%)")
        print(f"   - Rejected (1): {(y_train == 1).sum():,} ({(y_train == 1).mean()*100:.1f}%)")
        
        return X_train, X_test, y_train, y_test
    
    def train_models(self, X_train, X_test, y_train, y_test):
        """
        Train and evaluate machine learning models
        """
        print("\nğŸ¤– Phase 3: Predictive Modeling")
        print("=" * 40)
        
        # Initialize models
        models = {
            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
            'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100)
        }
        
        results = {}
        
        for model_name, model in models.items():
            print(f"\nğŸ”„ Training {model_name}...")
            
            # Train model
            model.fit(X_train, y_train)
            
            # Make predictions
            y_pred_train = model.predict(X_train)
            y_pred_test = model.predict(X_test)
            
            # Calculate metrics
            train_accuracy = accuracy_score(y_train, y_pred_train)
            test_accuracy = accuracy_score(y_test, y_pred_test)
            
            # Confusion matrix
            cm = confusion_matrix(y_test, y_pred_test)
            
            # Classification report
            class_report = classification_report(y_test, y_pred_test, output_dict=True)
            
            # Store results
            results[model_name] = {
                'model': model,
                'train_accuracy': train_accuracy,
                'test_accuracy': test_accuracy,
                'confusion_matrix': cm.tolist(),
                'classification_report': class_report
            }
            
            print(f"âœ… {model_name} Results:")
            print(f"   ğŸ“ˆ Training Accuracy: {train_accuracy:.4f}")
            print(f"   ğŸ“Š Test Accuracy: {test_accuracy:.4f}")
            print(f"   ğŸ¯ Generalization Gap: {(train_accuracy - test_accuracy):.4f}")
            
            print("\nğŸ“‹ Confusion Matrix:")
            print(f"   True Negative (Correct Accepted): {cm[0,0]:,}")
            print(f"   False Positive (Wrong Rejected): {cm[0,1]:,}")
            print(f"   False Negative (Wrong Accepted): {cm[1,0]:,}")
            print(f"   True Positive (Correct Rejected): {cm[1,1]:,}")
            
            print(f"\nğŸ“Š Classification Report:")
            print(f"   Precision (Accepted): {class_report['0']['precision']:.4f}")
            print(f"   Precision (Rejected): {class_report['1']['precision']:.4f}")
            print(f"   Recall (Accepted): {class_report['0']['recall']:.4f}")
            print(f"   Recall (Rejected): {class_report['1']['recall']:.4f}")
            print(f"   F1-Score (Macro Avg): {class_report['macro avg']['f1-score']:.4f}")
        
        self.models = results
        return results
    
    def generate_insights_and_recommendations(self):
        """
        Generate actionable insights and business recommendations
        """
        print("\nğŸ’¡ Business Insights and Recommendations")
        print("=" * 50)
        
        # Model performance insights
        best_model = max(self.models.keys(), 
                        key=lambda x: self.models[x]['test_accuracy'])
        best_accuracy = self.models[best_model]['test_accuracy']
        
        print("ğŸ† MODEL PERFORMANCE:")
        print(f"   Best Model: {best_model}")
        print(f"   Accuracy: {best_accuracy:.2%}")
        print(f"   Business Impact: Can correctly identify {best_accuracy:.1%} of risk cases")
        
        # Risk insights
        rejection_rate = self.analysis_results['dataset_overview']['rejection_rate']
        total_loans = self.analysis_results['dataset_overview']['total_records']
        
        print(f"\nğŸ“Š RISK PROFILE:")
        print(f"   Overall Rejection Rate: {rejection_rate:.2%}")
        print(f"   Total Applications Analyzed: {total_loans:,}")
        print(f"   High-Risk Applications: {int(total_loans * rejection_rate):,}")
        
        # Feature importance insights
        print(f"\nğŸ” KEY RISK FACTORS:")
        for i, factor in enumerate(self.analysis_results['risk_factors'][:5], 1):
            print(f"   {i}. {factor['feature']}")
            print(f"      - Risk Direction: {factor['risk_direction']} values = Higher risk")
            print(f"      - Predictive Strength: {factor['correlation']:.3f}")
        
        # Business recommendations
        recommendations = [
            "ğŸ¯ IMPLEMENTATION RECOMMENDATIONS:",
            f"   1. Deploy {best_model} for automated loan screening",
            "   2. Focus monitoring on top 5 risk factors identified",
            "   3. Implement tiered approval process based on risk scores",
            "   4. Set up real-time alerts for high-risk applications",
            "   5. Regularly retrain models with new loan performance data",
            "",
            "ğŸ’° BUSINESS VALUE:",
            f"   - Reduce manual review time by {best_accuracy:.0%}",
            "   - Improve risk detection accuracy",
            "   - Enable faster loan approval decisions",
            "   - Minimize potential default losses",
            "",
            "âš ï¸ RISK MANAGEMENT:",
            "   - Monitor model performance monthly",
            "   - Validate predictions with actual outcomes",
            "   - Maintain human oversight for edge cases",
            "   - Update risk thresholds based on business needs"
        ]
        
        print("\n" + "\n".join(recommendations))
        
        # Save insights
        insights_data = {
            'model_performance': {
                'best_model': best_model,
                'accuracy': float(best_accuracy),
                'all_models': {name: float(results['test_accuracy']) 
                             for name, results in self.models.items()}
            },
            'risk_profile': self.analysis_results['dataset_overview'],
            'key_risk_factors': self.analysis_results['risk_factors'][:5],
            'recommendations': recommendations
        }
        
        with open('financial_analysis_results/business_insights.json', 'w') as f:
            json.dump(insights_data, f, indent=2, default=str)
        
        print(f"\nâœ… Insights saved to 'financial_analysis_results/business_insights.json'")
        return insights_data
    
    def run_complete_analysis(self, sample_size=None):
        """
        Run the complete financial risk management analysis pipeline
        """
        print("ğŸš€ FINANCIAL RISK MANAGEMENT SYSTEM")
        print("=" * 60)
        print("Starting comprehensive risk analysis pipeline...")
        print()
        
        try:
            # Phase 1: Data Loading and Cleaning
            self.load_data(sample_size)
            if self.combined_data is None:
                return
            
            self.clean_data()
            if self.processed_data is None:
                return
            
            # Phase 2: Exploratory Data Analysis
            self.exploratory_data_analysis()
            
            # Phase 3: Predictive Modeling
            X_train, X_test, y_train, y_test = self.prepare_modeling_data()
            if X_train is None:
                return
            
            self.train_models(X_train, X_test, y_train, y_test)
            
            # Phase 4: Business Insights
            self.generate_insights_and_recommendations()
            
            print("\nğŸ‰ ANALYSIS COMPLETED SUCCESSFULLY!")
            print("=" * 50)
            print("ğŸ“ All results saved in 'financial_analysis_results' directory")
            print("ğŸ“Š Check the generated analysis files and reports")
            print("ğŸ’¡ Review business insights for actionable recommendations")
            print("\nğŸ”‘ KEY FILES GENERATED:")
            print("   - eda_summary.json: Exploratory data analysis results")
            print("   - business_insights.json: Business recommendations")
            print("\nğŸ¯ NEXT STEPS:")
            print("   1. Review the key risk factors identified")
            print("   2. Implement the recommended model in production")
            print("   3. Set up monitoring for ongoing risk assessment")
            print("   4. Plan regular model retraining schedule")
            
        except Exception as e:
            print(f"âŒ Error during analysis: {str(e)}")
            import traceback
            traceback.print_exc()

# Main execution
if __name__ == "__main__":
    # Initialize the Financial Risk Manager
    risk_manager = FinancialRiskManager(
        accepted_data_path="accepted_2007_to_2018q4.csv/accepted_2007_to_2018Q4.csv",
        rejected_data_path="rejected_2007_to_2018q4.csv/rejected_2007_to_2018Q4.csv"
    )
    
    # Run complete analysis
    # Using sample_size for faster processing with large datasets
    # For full analysis, remove the sample_size parameter
    print("ğŸ”§ Note: Using sample of 50,000 records for faster processing")
    print("ğŸ”§ For full dataset analysis, modify the sample_size parameter")
    print()
    
    risk_manager.run_complete_analysis(sample_size=50000)